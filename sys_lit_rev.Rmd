---
title: "Systematic Literature Review using litsearchr"
author: "K Wolcott"
date: "26 Dec 2023"
output: rmarkdown::github_document
always_allow_html: TRUE
---
<style>
table {
  background-color: white !important;
  color: black !important;
}
</style>

## Introduction
Use this notebook to carry out a systematic literature search using litsearchr on a Mac OS. We are investigating the morphology and evolution of the Byttnerioideae, a taxonomically challenging clade within the Malvaceae. First, you will use some manually selected keywords for preliminary searches. Then, keywords will be automatically extracted from the abstracts found during the preliminary passes. Finally, carry out a systematic literature search using the complete list of keywords and analyze patterns in found articles.

These steps were made following a tutorial from https://www.r-bloggers.com/2023/03/automated-systematic-literature-search-using-r-litsearchr-and-google-scholar-web-scraping/

## This is an R Markdown Document
Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

## Install and load packages and functions
```{r}
# Dependency packages to load/install
packages = c(
  "litsearchr", "dplyr", "stopwords","readr", "devtools", "kableExtra",
  "ggplot2", "ggraph", "ggrepel", "igraph", "rvest", "httr", "knitr"
)

# Install litsearchr using R devtools
cat("\nInstalling litsearchr package from GitHub ")
if(require(litsearchr)) remotes::install_github("elizagrames/litsearchr", ref = "main")

# Install packages not yet installed
installed_packages = packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
  install.packages("igraph", type="binary")
}

# Load packages
invisible(lapply(packages, library, character.only = TRUE))
cat("\nLoaded packages:\n", packages)

# List listsearchr version (will help with troubleshooting)
cat("\n\nlitsearchr version:")
packageVersion("litsearchr")

# Load in scrape_gs() function from GitHub to scrape Google Scholar for articles
devtools::source_url("https://gist.githubusercontent.com/ClaudiuPapasteri/7bef34394c395e03ee074f884ddbf4d4/raw/35c20268c3e3516d267ff2e8b96a4648195ed5ce/scrape_scholargooglesearch.R")
cat("\n\nLoading scrape_gs function from GitHub")
#View(scrape_gs) # Un-comment this line to inspect function in new tab
```

## Setup Markdown notebook parameters 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # Include all code blocks in knitted PDF output
```

## Set working directory
TO DO: Define your wd variable below
```{r}
wd = "/Users/katherinewolcott/Documents/r/jill" # TO DO: define your wd
cwd = paste(wd, "/litrev", sep="")
ifelse(!dir.exists(wd), c(cat("Creating directory", cwd), dir.create(cwd, recursive=TRUE)), c(cat("Directory already created", cwd, " "), FALSE))
knitr::opts_knit$set(root.dir = cwd)
cat("Working directory set to: ", cwd)
```

## TEST 1: Try out code to search Google Scholar and automatically extract new search terms
```{r, cache = TRUE}
# Run test query for 1 search phrase and the first 5 pages of Google Scholar results
## intext searches abstract instead of title
phrase1 = 'intext:"Byttnerioideae" AND "morphology"'  # Use Boolean operators
test1 = scrape_gs(term = phrase1, crawl_delay = 1.2, pages = 1:5) 
cat("Test 1: Query results from Google Scholar\n")
# Display the table as a scrollable object using kableExtra package
kbl(test1, table.attr = "style = \"color: black;\"") %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "100%")

# Test automatic term extraction from abstracts for the first 5 pages of results
gs_df = test1
test1_gs_terms = litsearchr::extract_terms(text = gs_df[,"abstract"],
                                      method = "fakerake", min_freq = 3, 
                                      min_n = 2,
                                      stopwords = 
                                        stopwords::data_stopwords_stopwordsiso$en
                                      )
cat("Test 1: Automatically extracted terms: \n")
print(test1_gs_terms)
```

## TEST 2: Try running multiple queries at the same time for different search term combinations
```{r, cache = TRUE}
# Run multiple queries using lists of higher taxa and key terms for the first 5 pages of Google Scholar search results
first  = list(term = 'intext:"Byttnerioideae" OR "Sterculiaceae" AND 
              "morphology"', pages = 1:5)
second = list(term = 'intext:"Byttnerioideae" OR "Sterculiaceae" AND
              "evolution"', pages = 1:5)
third = list(term = 'intext:"Byttnerioideae" OR "Sterculiaceae" AND "ecology"',
             pages = 1:5)
test_list = list(first, second, third) 
test2 = do.call(rbind, lapply(test_list, function(x) do.call(scrape_gs,
                                                             c(crawl_delay = 1.2,
                                                               x))))

# Display the table as a scrollable object using kableExtra package
cat("Test 2: Query results from Google Scholar\n")
kbl(test2, table.attr = "style = \"color: black;\"") %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "100%")

# Test automatic term extraction from abstracts for the first 5 pages of results
gs_df = test2
test2_gs_terms = litsearchr::extract_terms(text = gs_df[,"abstract"],
                                           method = "fakerake", min_freq = 3,
                                           min_n = 2,
                                           stopwords = 
                                      stopwords::data_stopwords_stopwordsiso$en)
cat("Test 2: Automatically extracted terms: \n")
print(test2_gs_terms)
```

# First pass: Define preliminary terms list for first full pass of Google Scholar scraping
TO DO: Modify lists of genera, families, and topics as needed
```{r}
# Combine Byttnerioideae genera, families, and topics of interest
genera = c("Hermannieae", "Lasiopetaleae", "Bytttnerieae", "Theobromeae", "Waltheria", "Hermannia", "Melochia", "Hannafordia", "Maxwellia", "Lasiopetalum", "Thomasia", "Guichenotia", "Commersonia",  "Rulingia", "Keraudrenia", "Seringia", "Abroma", "Byttneria", "Ayenia", "Rayleya", "Kleinhovia",  "Leptonychia", "Scaphopetalum", "Theobroma", "Herrania", "Glossostemon", "Guazuma") # TO DO: Add any missing genera, if any
families = c("Byttnerioideae", "Sterculiaceae") # Including old nomenclature
topics = c("morphology", "evolution", "ecology", "phylogenetics") # TO DO: Add any relevant topics
taxa = c(genera, families)

# Make an ordered list by family, genus, and topic
gs_grouped_selected_terms <- list(
taxa = taxa,
topics = topics
)

# Write the search string
search_string = litsearchr::write_search(
gs_grouped_selected_terms,
languages = "English",
exactphrase = TRUE,
stemming = FALSE,
closure = "left",
writesearch = FALSE
)

# Print the search string
cat("\nSearch string: \n")
print(search_string)
```

# First pass: Scrape Google Scholar
```{r, cache = TRUE}
# Define useragent
useragent = httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")

# Scrape pages 1:25 (250 papers) and save results as a tsv file (gs_df1.tsv)
gs_df1 = scrape_gs(term = 'intext:"Byttnerioideae" AND "morphology"', pages =
                     1:25, crawl_delay = 1.2, useragent) 
write.table(gs_df, file='gs_df1.tsv', quote=TRUE, sep='\t', row.names = FALSE)
```

# Create Co-Occurrence Network
```{r}
# Read in search results
gs_df1 = read.table("gs_df1.tsv", sep='\t', header = TRUE, fill = TRUE)

# Display results as a scrollable table using Kable
cat("Search results: \n")
kbl(gs_df1, table.attr = "style = \"color: black;\"") %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "100%")

# Remove duplicates
gs_df = gs_df1 %>% distinct(title, .keep_all = TRUE)
cat("\nRows in original df: ", nrow(gs_df1))
cat("\nRows in df with duplicates removed: ", nrow(gs_df))

# Combine key term results
gs_df_terms = litsearchr::extract_terms(text = gs_df[,"abstract"],
                                        method = "fakerake", 
                                        min_freq = 3, 
                                        min_n = 2,
                                        stopwords = 
                                        stopwords::data_stopwords_stopwordsiso$en
                                        )
cat("\nFirst pass: Automatically extracted terms: \n")
print(gs_df_terms)

# Remove duplicates
gs_df_terms_unq = unique(gs_df_terms)
cat("\nTerms in original list: ", length(gs_df_terms))
cat("\nTerms in list with duplicates removed: ", length(gs_df_terms_unq))
cat("\n")

# Use the title and abstract of each article
gs_docs = paste(gs_df[, "title"], gs_df[, "abstract"]) 
gs_dfm = litsearchr::create_dfm(elements = gs_docs, 
                                features = gs_df_terms_unq) # doc-feature matrix

# Create the coocurrence network
gs_coocnet = litsearchr::create_network(gs_dfm, min_studies = 3)

# Plot the network
coocnet_fig = ggraph(gs_coocnet, layout = "stress") +
              coord_fixed() +
              expand_limits(x = c(-1.5, 8)) +
              geom_edge_link(aes(alpha = weight)) +
              geom_node_point(shape = "circle filled", fill = "white") +
              geom_node_text(aes(label = name), hjust = "outward", check_overlap 
                             = TRUE) +
              guides(edge_alpha = "none") +
              theme_void()
              coocnet_fig
              dev.off()

# Save plot to png
cooc_fname = "cooc_net.png"
ggsave(coocnet_fig, file = cooc_fname, width = 8.5, height = 3, bg = "white")
cat("\nCo-ocurrence network plot saved to: ", cooc_fname)
coocnet_fig
```

# Prune the network based on node strength
```{r}
gs_node_strength = igraph::strength(gs_coocnet)
gs_node_rankstrength = data.frame(term = names(gs_node_strength), strength = gs_node_strength, row.names = NULL)
gs_node_rankstrength$rank = rank(gs_node_rankstrength$strength, ties.method = "min")
gs_node_rankstrength = gs_node_rankstrength[order(gs_node_rankstrength$rank),]
cooc_rank_fig = ggplot(gs_node_rankstrength, 
                          aes(x = rank, y = strength, label = term)) +
                          geom_line(lwd = 0.8) +
                          geom_point() +
                          ggrepel::geom_text_repel(size = 3, hjust = "right", 
                                                   nudge_y = 3, max.overlaps = 
                                                   30) +
                          theme_bw()
                    cooc_rank_fig
                    dev.off()

# Save plot to png
cooc_rank_fname = "cooc_ranked_strength.png"
ggsave(cooc_rank_fig, file = cooc_rank_fname, width = 8.5, height = 3, bg = "white")
cat("\nCo-ocurrence network plot saved to: ", cooc_rank_fname)
cooc_rank_fig
```


# Inspect extracted terms from first pass
```{r}
# Inspect terms
gs_terms = litsearchr::get_keywords(gs_coocnet)
cat("Identified terms: \n")
print(gs_terms)

# Save results to text file
gs_terms_fname = "identified_search_terms.txt"
write.table(gs_terms, file = gs_terms_fname, row.names = FALSE, 
            col.names = FALSE, sep = "\n")
cat("\nIdentified search terms saved to: ", gs_terms_fname)
```

# Manually add and remove substrings
# TO DO: Add/remove terms to list below
```{r}
# Manually remove all unwanted substrings 
to_remove = c("abroma agusta", "byttneria pilosa", "family sterculiaceae", "conservation status", "family sterculiaceae", "gross morphology", "heracleum lasiopetalum", "paper reports", "sympatric species", "tetrataenium lasiopetalum", "theobroma cacao", "waltheria indica", "waltheria species", "waltheria viscosissima") # TO DO: Modify this list based on terms displayed above that you do not want to search using
gs_selected_terms = gs_terms[!gs_terms %in% to_remove] 

# Manually add genera, families, and topics to include in the search
genera = c("Hermannieae", "Lasiopetaleae", "Bytttnerieae", "Theobromeae", "Waltheria", "Hermannia", "Melochia", "Hannafordia", "Maxwellia", "Lasiopetalum", "Thomasia", "Guichenotia", "Commersonia",  "Rulingia", "Keraudrenia", "Seringia", "Abroma", "Byttneria", "Ayenia", "Rayleya", "Kleinhovia",  "Leptonychia", "Scaphopetalum", "Theobroma", "Herrania", "Glossostemon", "Guazuma") # TO DO: Modify accordingly and check for spelling/typos
families = c("Byttnerioideae", "Sterculiaceae")
# Combine families and genera into one list
taxa = c(families, genera)

# Build topics list to contain ALL TOPICS to search for
topics = c("morphology", "evolution", "ecology", "phylogenetics", "pollen morphology", "floral morphology", "floral development", "floral characteristics", "pollen", "phylogeny", "medicinal plants", "chemical investigation") # TO DO: Add topics not automatically identified that you want to include
# Combine manually added and automatically extracted topics into one list
topics = c(topics, gs_selected_terms)

# Add manually selected terms to automatically extracted term list
to_add = c(taxa, topics)
final_terms = c(gs_selected_terms, to_add)

# Remove duplicates
final_terms = unique(final_terms)
cat("\nFinal list of terms: \n")
print(final_terms)

# Save results to text file 
final_terms_fname = "final_search_terms.txt"
write.table(final_terms, file = final_terms_fname, row.names = FALSE, 
            col.names = FALSE, sep = "\n")
cat("Identified search terms saved to: ", final_terms_fname)
```

# Automatically group into 3 searches
```{r}
# Check that all terms are included in a group (taxa or topics)
all.equal(length(final_terms), sum(length(taxa), length(topics)))
cat("\nIf lengths are not equal, double check terms added and removed in block above.\n")

# Split terms into three lists to do three different searches 
## This prevents errors when search phrases are too long
split_taxa = split(taxa, c(1:3)) # Split taxa list into 3 parts
for (i in 1:3){  
    taxa_x = paste("taxa", i, sep = "") 
    assign(taxa_x, split_taxa[[i]]) 
    cat("\nTaxa list ", i, "\n")
    print(get(taxa_x))
    rm(taxa_x)}

split_topics = split(topics, c(1:3)) # Split topics list into 3 parts
for (i in 1:3){
    topic_x = paste("topic", i, sep = "")
    assign(topic_x, split_topics[[i]])
    cat("\nTopics list ", i, "\n")
    print(get(topic_x))
    rm(topic_x)}

# Make a grouped list by family, genus, and topic
## Loop for searches 1 - 3
for (i in 1:3){
    f_grp_terms = paste("final_grouped_terms", i, sep = "")
    assign(f_grp_terms, list(taxa = get(paste("taxa", i, sep = "")),
                             topics = get(paste("topic", i, sep = ""))))
    cat("\nGrouped list ", i, "\n")
    print(get(f_grp_terms))
    rm(f_grp_terms)}
```

# Automatically write the search strings
```{r}
# Loop to write search strings 1-3
for (i in 1:3){
    # Name temporary variables
    search_str = paste("search_string", i, sep = "") 
    f_grp_terms = get(paste("final_grouped_terms", i, sep=''))
    # Assign variable values and generate search string
    cat("\n")
    assign(search_str, litsearchr::write_search(f_grp_terms, 
                                                languages = "English",
                                                exactphrase = TRUE,
                                                stemming = FALSE,
                                                closure = "left",
                                                writesearch = FALSE))
    # Print the generated search string
    cat("\nSearch string ", i, "\n") 
    print(get(search_str))
    # Remove special characters
    search_str = gsub("[^A-Za-z0-9 \\()]", "", get(search_str)) 
    cat("\nSearch string with special characters removed: \n")
    print(search_str)
    # Specify where to search for terms
    search_str = paste("intext:", search_str, sep = "")
    cat("\nSearch string with search location added (intext/intitle): \n")
    print(search_str)
    # Save search string to text file
    search_str_fname = paste("search_string", i, ".txt", sep = "")
    write.table(search_str, file = search_str_fname, row.names = FALSE, 
                col.names = FALSE, sep = "\n")
    cat("\nSearch string saved to: \n", search_str_fname)
    # Remove temporary loop variables
    rm(search_str, f_grp_terms)}
```

# Actual Search: Scrape Google Scholar using terms defined above in three passes
```{r, cache = TRUE}
# TO DO: Run this in three parts (over 3 days) by commenting/uncommenting out lines for gs_df_final1-3 and write.table(gs_df_final1-3.tsv)

# Define useragent
useragent = httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")

# Define search number (1-3)
search_num = "1" # TO DO: Change this value from 1-3 each time you run this block
cat("Performing Search ", search_num)

# Auto-fill other search parameters
## Do the search (Scrape pages 1:25 (250 papers))
gs_df_fx = paste("gs_df_final", search_num, sep = "")
cat("\nAssigning results to variable: ", gs_df_fx)
search_str = paste("search_string", search_num, sep = "")
assign(gs_df_fx, scrape_gs(term = search_str, pages = 1:25, crawl_delay = 
                             1.2, useragent))
## Export the results to tsv file (gs_df_final1-3.tsv)
gs_df_fname = paste("gs_df_final", search_num, ".tsv", sep = "")
cat("\nSaving results for search to: ", gs_df_fname)
write.table(get(gs_df_fx), file = gs_df_fname, quote = TRUE, sep = '\t',
            row.names = FALSE)
```

# Analyze search result patterns
Analyze search result patterns here and keep the resulting figures for your literature review paper. Then read through the articles and manually apply filter criteria yourself before writing the literature review.
```{r cache=FALSE}
# Read in search results
gs_df_f1 = read.table("gs_df_final1.tsv", sep='\t', header = TRUE)
gs_df_f2 = read.table("gs_df_final2.tsv", sep='\t', header = TRUE)
gs_df_f3 = read.table("gs_df_final3.tsv", sep='\t', header = TRUE)

# Concatenate results into one dataframe
rgs_df_f = rbind(gs_df_f1, gs_df_f2, gs_df_f3)

# Display results as a scrollable table using Kable
cat("\Search results: \n")
kbl(gs_df_f, table.attr = "style = \"color: black;\"") %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "100%")

# Remove duplicates
gs_df = gs_df_f %>% distinct(title, .keep_all = TRUE)
cat("\nRows in original df: ", nrow(gs_df_f))
cat("\nRows in df with duplicates removed: ", nrow(gs_df))

# Combine key term results
gs_df_terms = litsearchr::extract_terms(text = gs_df[,"abstract"],
                                        method = "fakerake", 
                                        min_freq = 3, 
                                        min_n = 2,
                                        stopwords = 
                                        stopwords::data_stopwords_stopwordsiso$en
                                        )
cat("First pass: Automatically extracted terms: \n")
print(gs_df_terms)

# Remove duplicates
gs_df_terms_unq = unique(gs_df_terms)
cat("\nTerms in original list: ", length(gs_df_terms))
cat("\nTerms in list with duplicates removed: ", length(gs_df_terms_unq))

# Create a document-feature matrix using article titles and abstracts
gs_docs = paste(gs_df[, "title"], gs_df[, "abstract"]) 
gs_dfm = litsearchr::create_dfm(elements = gs_docs, 
                                features = gs_df_terms_unq) 
# Create a co-occurence network for all terms found in 3+ articles
gs_coocnet = litsearchr::create_network(gs_dfm, min_studies = 3)
ggraph(gs_coocnet, layout = "stress") +
coord_fixed() +
expand_limits(x = c(-3, 3)) +
geom_edge_link(aes(alpha = weight)) +
geom_node_point(shape = "circle filled", fill = "white") +
geom_node_text(aes(label = name), hjust = "outward", check_overlap = TRUE) +
guides(edge_alpha = "none") +
theme_void()
```
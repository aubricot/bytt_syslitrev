---
title: "Systematic Literature Review using litsearchr"
author: "K Wolcott"
date: "16 Nov 2023"
output: rmarkdown::github_document
always_allow_html: TRUE
---
<style>
table {
  background-color: white !important;
  color: black !important;
}
</style>

## Introduction
Use this notebook to carry out a systematic literature search using litsearchr on a Mac OS. We are investigating the morphology and evolution of the Byttnerioideae, a taxonomically challenging clade within the Malvaceae. First, you will use some manually selected keywords for preliminary searches. Then, keywords will be automatically extracted from the abstracts found during the preliminary passes. Finally, carry out a systematic literature search using the complete list of keywords and analyze patterns in found articles.

These steps were made following a tutorial from https://www.r-bloggers.com/2023/03/automated-systematic-literature-search-using-r-litsearchr-and-google-scholar-web-scraping/

## This is an R Markdown Document
Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

## Install and load packages and functions
```{r}
# Dependency packages to load/install
packages = c(
  "litsearchr", "dplyr", "stopwords","readr", "devtools", "kableExtra",
  "ggplot2", "ggraph", "ggrepel", "igraph", "rvest", "httr", "knitr"
)

# Install litsearchr using R devtools
cat("\nInstalling litsearchr package from GitHub ")
if(require(litsearchr)) remotes::install_github("elizagrames/litsearchr", ref = "main")

# Install packages not yet installed
installed_packages = packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
  install.packages("igraph", type="binary")
}

# Load packages
invisible(lapply(packages, library, character.only = TRUE))
cat("\nLoaded packages:\n", packages)

# List listsearchr version (will help with troubleshooting)
cat("\n\nlitsearchr version:")
packageVersion("litsearchr")

# Load in scrape_gs() function from GitHub to scrape Google Scholar for articles
devtools::source_url("https://gist.githubusercontent.com/ClaudiuPapasteri/7bef34394c395e03ee074f884ddbf4d4/raw/35c20268c3e3516d267ff2e8b96a4648195ed5ce/scrape_scholargooglesearch.R")
cat("\n\nLoading scrape_gs function from GitHub")
#View(scrape_gs) # Un-comment this line to inspect function in new tab
```

## Setup Markdown notebook parameters 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # Include all code blocks in knitted PDF output
```

## Set working directory
TO DO: Define your wd variable below
```{r}
wd = "/Users/katherinewolcott/Documents/r/jill" # TO DO: define your wd
cwd = paste(wd, "/litrev", sep="")
ifelse(!dir.exists(wd), c(cat("Creating directory", cwd), dir.create(cwd, recursive=TRUE)), c(cat("Directory already created", cwd, " "), FALSE))
knitr::opts_knit$set(root.dir = cwd)
cat("Working directory set to: ", cwd)
```

## TEST 1: Try out code to search Google Scholar and automatically extract new search terms
```{r, cache = TRUE}
# Run test query for 1 search phrase and the first 5 pages of Google Scholar results
## intext searches abstract instead of title
phrase1 = 'intext:"Byttnerioideae" AND "morphology"'  # Use Boolean operators
test1 = scrape_gs(term = phrase1, crawl_delay = 1.2, pages = 1:5) 
cat("Test 1: Query results from Google Scholar\n")
# Display the table as a scrollable object using kableExtra package
kbl(test1, table.attr = "style = \"color: black;\"") %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "100%")

# Test automatic term extraction from abstracts for the first 5 pages of results
gs_df = test1
test1_gs_terms = litsearchr::extract_terms(text = gs_df[,"abstract"],
                                      method = "fakerake", min_freq = 3, min_n = 2,
                                      stopwords = stopwords::data_stopwords_stopwordsiso$en
                                      )
cat("Test 1: Automatically extracted terms: \n")
print(test1_gs_terms)
```

## TEST 2: Try running multiple queries at the same time for different search term combinations
```{r, cache = TRUE}
# Run multiple queries using lists of higher taxa and key terms for the first 5 pages of Google Scholar search results
first  = list(term = 'intext:"Byttnerioideae" OR "Sterculiaceae" AND "morphology"', pages = 1:5)
second = list(term = 'intext:"Byttnerioideae" OR "Sterculiaceae" AND "evolution"', pages = 1:5)
third = list(term = 'intext:"Byttnerioideae" OR "Sterculiaceae" AND "ecology"', pages = 1:5)
test_list = list(first, second, third) 
test2 = do.call(rbind, lapply(test_list, function(x) do.call(scrape_gs, c(crawl_delay = 1.2, x))))
cat("Test 2: Query results from Google Scholar\n")
# Display the table as a scrollable object using kableExtra package
kbl(test2, table.attr = "style = \"color: black;\"") %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "100%")

# Test automatic term extraction from abstracts for the first 5 pages of results
gs_df = test2
test2_gs_terms = litsearchr::extract_terms(text = gs_df[,"abstract"],
                                           method = "fakerake", min_freq = 3, min_n = 2,
                                           stopwords = stopwords::data_stopwords_stopwordsiso$en)
cat("Test 2: Automatically extracted terms: \n")
print(test2_gs_terms)
```

# TEST 3: Try many different search term combinations for all genera within the Byttnerioideae
```{r, cache = TRUE}
# Run multiple queries using lists of specific genera in the Byttnerioideae
first  = list(term = 'intext:"Hermannieae" OR "Lasiopetaleae" OR "Bytttnerieae" OR "Theobromeae"', pages = 1:5)
second = list(term = 'intext:"Waltheria" OR "Hermannia" OR "Melochia" OR "Hannafordia" OR "Maxwellia"', pages = 1:5)
third = list(term = 'intext:"Lasiopetalum" OR "Thomasia" OR "Guichenotia" OR "Commersonia" OR "Rulingia"', pages = 1:5)
fourth = list(term = 'intext:"Keraudrenia" OR "Seringia" OR "Abroma" OR "Byttneria" OR "Ayenia" OR "Rayleya"', pages = 1:5)
fifth = list(term = 'intext:"Kleinhovia" OR "Leptonychia" OR "Scaphopetalum" OR "Theobroma" OR "Herrania" OR "Glossostemon" OR "Guazuma"', pages = 1:5)
test_list = list(first, second, third, fourth, fifth) 

test3 = do.call(rbind, lapply(test_list, function(x) do.call(scrape_gs, c(crawl_delay = 1.2, x))))
cat("Test 3: Query results from Google Scholar\n")
# Display the table as a scrollable object using kableExtra package
kbl(test3, table.attr = "style = \"color: black;\"") %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "100%")

# Test automatic term extraction from abstracts for the first 5 pages of results
gs_df = test3
test3_gs_terms = litsearchr::extract_terms(text = gs_df[,"abstract"],
                                           method = "fakerake", min_freq = 3, 
                                           min_n = 2,
                                           stopwords = 
                                        stopwords::data_stopwords_stopwordsiso$en
                                      )
cat("Test 3: Automatically extracted terms: \n")
print(test3_gs_terms)
```

# First pass: Define preliminary terms list for first full pass of Google Scholar scraping
```{r}
# Combine Byttnerioideae genera, families, and topics of interest
genera = c("Hermannieae", "Lasiopetaleae", "Bytttnerieae", "Theobromeae", "Waltheria", "Hermannia", "Melochia", "Hannafordia", "Maxwellia", "Lasiopetalum", "Thomasia", "Guichenotia", "Commersonia",  "Rulingia", "Keraudrenia", "Seringia", "Abroma", "Byttneria", "Ayenia", "Rayleya", "Kleinhovia",  "Leptonychia", "Scaphopetalum", "Theobroma", "Herrania", "Glossostemon", "Guazuma") # TO DO: Add any missing genera, if any
families = c("Byttnerioideae", "Sterculiaceae") # Including old nomenclature
topics = c("morphology", "evolution", "ecology", "phylogenetics") # TO DO: Add any relevant topics
taxa = c(genera, families)

# Make an ordered list by family, genus, and topic
gs_grouped_selected_terms <- list(
taxa = taxa,
topics = topics
)

# Write the search
search_string = litsearchr::write_search(
gs_grouped_selected_terms,
languages = "English",
exactphrase = TRUE,
stemming = FALSE,
closure = "left",
writesearch = FALSE
)

cat("\nSearch string: \n")
print(search_string)
```

# First pass: Scrape Google Scholar
```{r, cache = TRUE}
# Define useragent
useragent = httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")

# Scrape pages 1:25 and save results as a tsv file (gs_df1.tsv)
gs_df1 = scrape_gs(term = 'intext:"Byttnerioideae" AND "morphology"', pages = 1:25, crawl_delay = 1.2, useragent) # scrape first 25 pages (250 published works)
write.table(gs_df, file='gs_df1.tsv', quote=TRUE, sep='\t', row.names = FALSE)
```

# Create Co-Occurrence Network
```{r}
# Read in search results
gs_df1 = read.table("gs_df1.tsv", sep='\t', header = TRUE)

# Display results as a scrollable table using Kable
cat("Search results: \n")
kbl(gs_df1, table.attr = "style = \"color: black;\"") %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "100%")

# Remove duplicates
gs_df = gs_df1 %>% distinct(title, .keep_all = TRUE)
cat("\nRows in original df: ", nrow(gs_df1))
cat("\nRows in df with duplicates removed: ", nrow(gs_df))

# Combine key term results
gs_df_terms = litsearchr::extract_terms(text = gs_df[,"abstract"],
                                        method = "fakerake", 
                                        min_freq = 3, 
                                        min_n = 2,
                                        stopwords = 
                                        stopwords::data_stopwords_stopwordsiso$en
                                      )
cat("\nFirst pass: Automatically extracted terms: \n")
print(gs_df_terms)

# Remove duplicates
gs_df_terms_unq = unique(gs_df_terms)
cat("\nTerms in original list: ", length(gs_df_terms))
cat("\nTerms in list with duplicates removed: ", length(gs_df_terms_unq))
cat("\n")

# Use the title and abstract of each article
gs_docs = paste(gs_df[, "title"], gs_df[, "abstract"]) 
gs_dfm = litsearchr::create_dfm(elements = gs_docs, 
                                features = gs_df_terms_unq
                                ) # document-feature matrix

# Create the coocurrence network
gs_coocnet = litsearchr::create_network(gs_dfm, min_studies = 3)

# Plot the network
coocnet_fig = ggraph(gs_coocnet, layout = "stress") +
              coord_fixed() +
              expand_limits(x = c(-1.5, 8)) +
              geom_edge_link(aes(alpha = weight)) +
              geom_node_point(shape = "circle filled", fill = "white") +
              geom_node_text(aes(label = name), hjust = "outward", check_overlap 
                             = TRUE) +
              guides(edge_alpha = "none") +
              theme_void()
              coocnet_fig
              dev.off()

# Save plot to png
cooc_fname = "cooc_net.png"
ggsave(coocnet_fig, file = cooc_fname, width = 8.5, height = 3, bg = "white")
cat("\nCo-ocurrence network plot saved to: ", cooc_fname)
coocnet_fig
```

# Prune the Network based on node strength
```{r}
gs_node_strength = igraph::strength(gs_coocnet)
gs_node_rankstrength = data.frame(term = names(gs_node_strength), strength = gs_node_strength, row.names = NULL)
gs_node_rankstrength$rank = rank(gs_node_rankstrength$strength, ties.method = "min")
gs_node_rankstrength = gs_node_rankstrength[order(gs_node_rankstrength$rank),]
cooc_rank_fig = ggplot(gs_node_rankstrength, 
                          aes(x = rank, y = strength, label = term)) +
                          geom_line(lwd = 0.8) +
                          geom_point() +
                          ggrepel::geom_text_repel(size = 3, hjust = "right", 
                                                   nudge_y = 3, max.overlaps = 
                                                   30) +
                          theme_bw()
                    cooc_rank_fig
                    dev.off()

# Save plot to png
cooc_rank_fname = "cooc_ranked_strength.png"
ggsave(cooc_rank_fig, file = cooc_rank_fname, width = 8.5, height = 3, bg = "white")
cat("\nCo-ocurrence network plot saved to: ", cooc_rank_fname)
cooc_rank_fig
```


# Inspect the terms
```{r}
# Inspect terms
gs_terms = litsearchr::get_keywords(gs_coocnet)
cat("Identified terms: \n")
print(gs_terms)

# Save results to text file
gs_terms_fname = "identified_search_terms.txt"
write.table(gs_terms, file = gs_terms_fname, row.names = FALSE, 
            col.names = FALSE, sep = "\n")
cat("Identified search terms saved to: ", gs_terms_fname)
```

# Manually add and remove substrings
# TO DO
```{r}
# Manually remove all unwanted substrings 
to_remove = c("subfamilia byttnerioideae", "study aimed", "abroma augusta", "family sterculiaceae", "waltheria indica", "waltheria douradinha", "theobroma cacao") # TO DO: Modify this list based on terms displayed above that you do not want to search using
gs_selected_terms = gs_selected_terms[!gs_selected_terms %in% to_remove] 
genera = c("Hermannieae", "Lasiopetaleae", "Bytttnerieae", "Theobromeae", "Waltheria", "Hermannia", "Melochia", "Hannafordia", "Maxwellia", "Lasiopetalum", "Thomasia", "Guichenotia", "Commersonia",  "Rulingia", "Keraudrenia", "Seringia", "Abroma", "Byttneria", "Ayenia", "Rayleya", "Kleinhovia",  "Leptonychia", "Scaphopetalum", "Theobroma", "Herrania", "Glossostemon", "Guazuma") # TO DO: Add any missing genera, if any
families = c("Byttnerioideae", "Sterculiaceae")
topics = c("morphology", "evolution", "ecology", "phylogenetics") # TO DO: Add any relevant topics
to_add = c(genera, families, topics)
final_terms = c(gs_selected_terms, to_add)
cat("\nFinal list of terms: \n")
print(final_terms)

# Save results to text file 
final_terms_fname = "final_search_terms.txt"
write.table(final_terms, file = final_terms_fname, row.names = FALSE, 
            col.names = FALSE, sep = "\n")
cat("Identified search terms saved to: ", final_terms_fname)
```

# Manual grouping into clusters - for more rigorous search we will need a combination of OR and AND operators
```{r}
topics = c(topics, final_terms[c(1:7)]) # TO DO: Make sure the indices for all returned topics are shown here
taxa = c(families, genera)

# Check that all terms were grouped
all.equal(length(final_terms), sum(length(taxa), length(topics)))

# Make an ordered list by family, genus, and topic
final_grouped_terms <- list(
taxa = taxa,
topics = topics
)
```

# Automatically write the search string
```{r}
# Write the search
search_string = litsearchr::write_search(
final_grouped_terms,
languages = "English",
exactphrase = TRUE,
stemming = FALSE,
closure = "left",
writesearch = FALSE
)

# Print the generated search string
cat("\nSearch string: \n")
print(search_string)
# Remove all characters from search string except letters, numbers, space, and parantheses
search_string = gsub("[^A-Za-z0-9 ()]", "", search_string)
cat("\nSearch string with special characters removed: \n")
print(search_string)

# Save search string to text file
search_string_fname = "search_string.txt"
write.table(search_string, file = search_string_fname, row.names = FALSE, 
            col.names = FALSE, sep = "\n")
cat("Identified search terms saved to: ", search_string_fname)
```

# Actual Search: Scrape Google Scholar using terms defined above
```{r, cache = TRUE}
# Define useragent
useragent = httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")

# Scrape pages 1:25 and save results as a tsv file (gs_df_final.tsv)
gs_df_final = scrape_gs(term = search_string, pages = 1:25, crawl_delay = 1.2, useragent) # scrape first 25 pages (250 published works)
write.table(gs_df_final, file='gs_df_final.tsv', quote=TRUE, sep='\t', row.names = FALSE)
```

# Analyze search result patterns
Analyze search result patterns here and keep the resulting figures for your literature review paper. Then read through the articles and manually apply filter criteria yourself before writing the literature review.
```{r, cache = TRUE}
# Read in search results
gs_df_f = read.table("gs_df_final.tsv", sep='\t', header = TRUE)

# Display results as a scrollable table using Kable
cat("\Search results: \n")
kbl(gs_df_f, table.attr = "style = \"color: black;\"") %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "100%")

# Remove duplicates
gs_df = gs_df_f %>% distinct(title, .keep_all = TRUE)
cat("\nRows in original df: ", nrow(gs_df_f))
cat("\nRows in df with duplicates removed: ", nrow(gs_df))

# Combine key term results
gs_df_terms = litsearchr::extract_terms(text = gs_df[,"abstract"],
                                        method = "fakerake", 
                                        min_freq = 3, 
                                        min_n = 2,
                                        stopwords = 
                                        stopwords::data_stopwords_stopwordsiso$en
                                      )
cat("First pass: Automatically extracted terms: \n")
print(gs_df_terms)

# Remove duplicates
gs_df_terms_unq = unique(gs_df_terms)
cat("\nTerms in original list: ", length(gs_df_terms))
cat("\nTerms in list with duplicates removed: ", length(gs_df_terms_unq))

# Use the title and abstract of each article
gs_docs = paste(gs_df[, "title"], gs_df[, "abstract"]) 
gs_dfm = litsearchr::create_dfm(elements = gs_docs, 
                                features = gs_df_terms_unq
                                ) # document-feature matrix
gs_coocnet = litsearchr::create_network(gs_dfm, min_studies = 3)
ggraph(gs_coocnet, layout = "stress") +
coord_fixed() +
expand_limits(x = c(-3, 3)) +
geom_edge_link(aes(alpha = weight)) +
geom_node_point(shape = "circle filled", fill = "white") +
geom_node_text(aes(label = name), hjust = "outward", check_overlap = TRUE) +
guides(edge_alpha = "none") +
theme_void()
```